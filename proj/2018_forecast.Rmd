---
title: "Pooling the polls to forecast the 2018 US House elections"
author: "Nick Ahamed"
date: "2/1/2018"
output:
  html_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include = F}
#Part 2
# -Apply uniform swing from 2016 estimate to 2016 results estimate share of vote for each district
# -Use SDs to calculate tipping point districts 
# -Use SDs to calculate p(victory) in tipping point districts
```
#Pooling the polls to forecast the 2018 US House elections
##Introduction
Using previously estimated pollster and sampling universe biases and current polling data, I forecast the the results of the 2018 US House elections. The Bayesian model predicts Democrats will get 55% of the popular vote, yielding 230 seats in the House of Representatives; current polling suggests there is a 57% chance Democrats will take back the House, on Election Day. Lastly, I apply a uniform swing from 2016 to identify key swing districts for investment in 2018. 

##Data
The polling data were scraped from Real Clear Politics' database for the [__2018__](https://www.realclearpolitics.com/epolls/other/2018_generic_congressional_vote-6185.html#polls) Generic Congressional ballot. Each pollster has a slightly different wording (and hence why we measure pollster bias), but they are all similar to: 'If the elections for the U.S. House of Representatives were being held today, which partyâ€™s candidate would you vote for in your congressional district: The Democratic candidate or the Republcian candidate?'  The named Congressional ballot question would account for incumbency effects and more closely mirror the choice voters are making in the voting booth. However, since not all candidates are known for 2018 yet, this is the most common question being polled by public sources. 

Only polls where the year, date range, pollster, sampling universe and sample size are all known were included. Additionally, the polls' results were transformed to reflect the two-way share for Democrats (Dem/(Dem+Rep)): it is a proportion between 0 and 1. Time is transformed to be the rounded number of weeks between the middle day of the poll and election day. A daily model would be more precise, but would take more data. Additionally, only sampling universes and pollsters who have polled in at least one of the last six election cycles are included. This is because I use estimates of bias observed over the last electios to account for house effects. See more [__here__](https://github.com/nickahamed/BSTM/blob/master/proj/pollster_bias.html).

As of 2/3/2018, 102 polls from 13 pollsters contacting 127k respondents were used. These are the 5 largest pollsters. See Appendix B for full details.

<center>
![](figures/2018_data_breakdown.png){ width=6in }
</center>

2016 US House election results were taken from [__here__](https://github.com/Prooffreader/election_2016_data).

##Week-by-week estimate of support
I use a Bayesian random-walk model to estimate the true level of support over time. Below, I show that model. Based on the most current data, the forecast gives Democrats 54.9% support on the generic ballot, translating to 230 seats in the House of Representatives. Likewise, there is relatively little volatility in week-to-week movements. Whereas in the past six election cycles, 95% of week-to-week movement was within 1.5pp, this cycle, 95% of movement is within just 0.66pp. We saw a maximum amount of support for Democrats right at the end of 2017, possibly associated with XXX event, but there's been a slight return since. 

<center>
![](figures/2018_time_series_with_trend.png){ width=8in }
</center>

If the election were held today, there is a 100% chance that Democrats win over 50% of the popular vote, and more importantly, there is nearly a 100% chance that they win a majority of seats in the House itself. Accounting for the variation we expect over time, there is still a 64% chance Democrats will win 50% of the popular vote and a 57% chance of winning a majority of seats __on election day__.

##Strategic targeting for 2018

#Appendix A
To specify my random walk model, I follow [__Jackman (2005)__](http://eppsac.utdallas.edu/files/jackman/CAJP%2040-4%20Jackman.pdf). A given poll is assumed to be normally distruted with support as the mean and the standard deviation a function of $y_i$ and sample size. This would be specified as:
$$y_i \sim \mathcal{N}(\mu_i, \sigma^2_i)$$
That poll is centered around mean $\mu_i$, which itself is a function of $\alpha_t$, the true value of support at the time the poll was taken $t$, $\delta_j$, the bias of pollster $j$, and $\theta_k$, the bias of sampling universe $k$. Fully specified, this is: 
$$\mu_i = \alpha_{t_i} + \delta_{j_i} + \theta_{k_i}$$
Due to the trends we see in our initial data exploration, a random walk model is appropriate. In such a model, support at time $t$ is normally distributed around support at time $t - 1$. 
$$ \alpha_t \sim \mathcal{N}(\alpha_{t-1}, \omega^2) $$
For these given specifications, we start with the following priors: 
$$ \sigma^2_i = \frac{y_i(1-y_i)}{n_i},\ \ \ \alpha_1 \sim \mathcal{U}(0.46, 0.56),\ \ \ \omega \sim \mathcal{U}(0, (0.02/1.96))$$
$\sigma^2_i$ just follows the formula for variance of a sample. As a prior for the starting true value of support ($\alpha_1$), I use a uniform distribution over the minimum and maximum actual vote share of Democrats in the six elections analyzed. Lastly, as a prior for the true standard deviation ($\omega$), I use a uniform distribution between 0 and 0.01. A value of 0.01 would reflect that 95% of week-to-week movement is within about 2pp in either direction, a fairly weak assumption. These priors are similar to [__Strauss (2007)__](http://www.mindlessphilosopher.net/princeton/strauss_reverse%20random%20walk.pdf) 

Unlike [__Jackman (2005)__](http://eppsac.utdallas.edu/files/jackman/CAJP%2040-4%20Jackman.pdf) and [__Strauss (2007)__](http://www.mindlessphilosopher.net/princeton/strauss_reverse%20random%20walk.pdf), I have strong priors for $\delta_j$ and $\theta_k$. Whereas they use a weak prior centered around 0, I use the best estimate from past election cycles and the observed uncertainty about that estimate. See more about the estimation of those [__here__](https://github.com/nickahamed/BSTM/blob/master/proj/pollster_bias.html), including the original priors and observed posteriors. However, using priors for all of the variables leaves the model underspecified. It is impossible to simultaneously estimate the bias for universe/pollster, and the true level of support each week. As such, I fix the prior for the pollster/universe that we have the least uncertainty about at it's bias estimate (set variance of prior = 0). Whereas past bias estimates were derived in relation to the true election result, this approach updates the priors in relation to this pollster/universe.

#Appendix B
##Load packages, functions and other setup 
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(rjags)
library(cowplot)
library(flextable)

source("forecasting_functions.R")

set.seed(102)
scipen=9
```

##Load, prep and explore data
```{r, message=FALSE, warning=FALSE}
pollster_lkup <- read.csv("data/pollster_lkup.csv")

deltas <- read.csv("data/final_pollster_bias_ests.csv")
thetas <- read.csv("data/final_universe_bias_ests.csv")
coefs <- read.csv("data/forecast_seats_coefs.csv")

polls <- read.csv("data/2018_polling.csv") %>%
  filter(pollster %in% pollster_lkup$pollster) %>%
  mutate(twoway = dem/(dem+rep)) %>% 
  mutate(week = round(as.numeric((as.Date(as.character("11/6/18"),  format="%m/%d/%y") - 
           as.Date(as.character(end_date),  format="%m/%d/%y")) + 
           (as.Date(as.character(end_date),  format="%m/%d/%y") - 
           as.Date(as.character(start_date),  format="%m/%d/%y"))/2)/7),
         n_size = as.numeric(as.character(n_size)))

polling_summary <- polls %>% 
  group_by(pollster) %>%
  summarise(`Total N-Size` = sum(n_size), 
            `# of Polls` = n()) %>%
  arrange(desc(`Total N-Size`)) %>%
  inner_join(pollster_lkup, by = "pollster") %>%
    mutate(pollster_raw = factor(pollster_raw, levels = pollster_raw[order(`Total N-Size`)]))

polling_summary_ft <- polling_summary %>% 
  mutate(nsize = as.character(`Total N-Size`),
         polls = `# of Polls`) %>%
  select(pollster_raw, nsize, polls)

FT1 <- flextable(polling_summary_ft)
FT1 <- set_header_labels(FT1, pollster_raw = "Pollster", nsize = "Total N-Size", polls = "# of Polls")

FT1 <- theme_zebra(x = FT1, odd_header = "#CFCFCF", odd_body = "#EFEFEF",
even_header = "transparent", even_body = "transparent")
FT1 <- align(x = FT1, j = 1, align = "left", part = "all")
FT1 <- align(x = FT1, j = 2:3, align = "center", part = "all")
FT1 <- bold(x = FT1, bold = TRUE, part = "header")
FT1
```

```{r, include=FALSE}
#Plots for above
x1 <- ggplot(data = (polling_summary[1:5,]), aes(y = `# of Polls`, x = pollster_raw)) +
  geom_bar(stat = 'identity') +
  coord_flip() + 
  theme_bw() + 
  xlab("") 

x2 <- ggplot(data = (polling_summary[1:5,]), aes(y = `Total N-Size`, x = pollster_raw)) +
  geom_bar(stat = 'identity') +
  coord_flip() + 
  theme_bw() + 
  xlab("") + 
  theme(axis.text.y = element_blank())

ggsave(filename = "figures/2018_data_breakdown.png", plot = plot_grid(x1, x2, nrow = 1), width = 8, height = 4, units = "in")
```

##Estimate week-by-week movement using past pollster and universe biases
```{r, warning=FALSE, message=FALSE, results=FALSE}
data_jags <- data_prep(data = polls, res = res, year = 2018, anchor = F)
data_jags <- bias_priors(data_jags = data_jags, deltas = deltas, thetas = thetas, anchor = F)

convergence_2018 <- convergence_diagnostics(data_jags = data_jags,
                                            anchor = F,
                                            chains = 4, 
                                            thining = 10, 
                                            burnin = 10000, 
                                            iter = 1000000)

mod_res <- run_model(data_jags = data_jags, 
                     params = c("xi", "omega"), 
                     anchor = F,
                     chains = 4, 
                     thining = 10, 
                     burnin = 10000, 
                     iter = 1000000)
cycle_time_est <- extract_time_est(mod_res = mod_res, year = 2018, data_jags = data_jags) %>%
  mutate(time_before_elec = time_before_elec + (max(data_jags$week) - max(data_jags$week_adj)))
omega <- extract_omega_est(mod_res = mod_res, year = 2018, data_jags = data_jags)
```

```{r, include=FALSE}
time_series_with_trend <- ggplot(data=cycle_time_est, aes(x=time_before_elec, y=iter_mean)) + 
  geom_point(data=polls, aes(x=week, y=twoway, size=sqrt(n_size)), alpha=0.2) +
  geom_ribbon(aes(ymin=lower_bound,ymax=upper_bound), alpha = 0.5) +
  geom_line(color = "blue",size = 0.75) +
  theme_bw() + 
  scale_x_reverse(name = "Weeks Before Election", limits= c(94, 0)) +
  scale_y_continuous(name = "Democratic Two-way Vote Share", labels=scales::percent, limits = c(min(0.48, min(polls$twoway) - 0.01), max(0.6, max(polls$twoway) + 0.01))) + 
  guides(size=F, color = F) + 
  geom_hline( aes(yintercept = 0.5), linetype="dashed") 

ggsave(filename = "figures/2018_time_series_with_trend.png", plot = time_series_with_trend, width = 8, height = 4, units = "in")
```

##Predict # of seats won and probability of taking back the House
```{r, warning=FALSE}
message(paste0("Convergance diagnostics for sample 2018 parameters:"))
print(convergence_2018$gelman)
print(convergence_2018$autocorr)

message(paste0("95% CI for week-to-week movement:"))
scales::percent(round(as.numeric(omega)*1.96,4))

##If the election were held today
mod_csim <- as.mcmc(do.call(rbind, mod_res))
mod_csim <- as.data.frame(mod_csim)
final_param <- paste0("xi[",(length(names(mod_csim))-1),"]")
final_forecast_posterior <- mod_csim[,final_param]

message(paste0("Current forecast of support:"))
scales::percent(round(mean(final_forecast_posterior),3))

message(paste0("Probability of 50% popular vote today:"))
scales::percent(round(mean(final_forecast_posterior > 0.5),3))

predict_seats <- round((coefs[1,1] + coefs[2,1]*mean(final_forecast_posterior))*435)
message(paste0("Number of seats estimated today:"))
predict_seats

final_seats_posterior <- round((coefs[1,1] + coefs[2,1]*final_forecast_posterior)*435)
message(paste0("Probability of a majority of seats today:"))
scales::percent(round(mean(final_seats_posterior > 217),3))


##Adding in error over time
new_means <- rnorm(100000, mean = mean(final_forecast_posterior), sd = (as.numeric(omega)*min(cycle_time_est$time_before_elec)))
new_seats <- round((coefs[1,1] + coefs[2,1]*new_means)*435)

message(paste0("Probability of 50% popular vote on election day:"))
scales::percent(round(mean(new_means > 0.5),3))

message(paste0("Probability of a majority of seats on election day:"))
scales::percent(round(mean(new_seats > 217),3))
```

