---
title: "Bayesian Inference of Pollster Bias"
output: pdf_document
author: Nick Ahamed
---

```{r setup, include=FALSE}
#Structure
# Part 1 - Inference
# -add a table of contents
# -make sure plots are correctly sized
# -Make flextables look nice
# -put math/specs of model in appendix a

#Part 2
# -Use final bias estimates as priors for an unanchored random walk model
# -Use regression from part 1 to estimate #seats won and use sd of model to estimate p(take back the house today) and p(take back the house in X weeks)
# -Plot random walk against polls with 95% CI
# -Apply uniform swing from 2016 estimate to 2016 results estimate share of vote for each district
# -Use SDs to calculate tipping point districts 
# -Use SDs to calculate p(victory) in tipping point districts
# -put math/specs of model in appendix a
# -put code in appendix b
```
##Introduction
In this analysis, I estimate the bias for each public pollster active in the last 6 congressional elections. My final estimate identifies X as the most conservative pollster and Y as the most liberal. I likewise estimate bias of various sampling universes. Next, I use these biases to estimate the true level of support for Democrats over time in each cycle. Lastly, I regress the final estimate of support in each cycle against the number of seats Democrats won.  

##The Data
I have two primary sources of data: past polls and election results. The poll response that I use is the 'generic Congressional ballot.' Each pollster has a slightly different wording (and hence why we measure pollster bias), but they are all similar to: 'If the elections for the U.S. House of Representatives were being held today, which partyâ€™s candidate would you vote for in your congressional district: The Democratic candidate or the Republcian candidate?'  The named Congressional ballot question would account for incumbency effects and more closely mirror the choice voters are making in the voting booth. However, since not all candidates are known for 2018 yet, this is the only current question being polled, and so for comparability, I will use the same question for past elections.

The past polls were taken from Real Clear Politics' database across 6 election cycles: [__2006__](https://realclearpolitics.com/epolls/other/2006_generic_congressional_vote-2174.html), [__2008__](https://www.realclearpolitics.com/epolls/other/2008_generic_congressional_vote-2173.html#polls), [__2010__](https://www.realclearpolitics.com/epolls/other/2010_generic_congressional_vote-2171.html#polls), [__2012__](https://www.realclearpolitics.com/epolls/other/2012_generic_congressional_vote-3525.html#polls), [__2014__](https://www.realclearpolitics.com/epolls/other/generic_congressional_vote-2170.html) and [__2016__](https://www.realclearpolitics.com/epolls/other/2016_generic_congressional_vote-5279.html#polls). Only polls where the year, date range, pollster, sampling universe and sample size are all known were included. Additionally, the polls' results were transformed to reflect the two-way share for Democrats (Dem/(Dem+Rep)): it is a proportion between 0 and 1. Time is transformed to be the rounded number of weeks between the middle day of the poll and election day. A daily model would be more precise, but would take more data.

In total, 797 polls from 41 pollsters contacting 1.7m respondents over the 6 election cycles were used. These are the 5 largest pollsters. See Appendix B for full details.

<center>
![](figures/data_breakdown.png)
</center>

For election results, I use both the popular vote share and the seats won. These were taken from Wikipedia: [__2006__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2006), [__2008__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2008), [__2010__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2010), [__2012__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2012), [__2014__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2014), and [__2016__](https://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2016). Again, I use Democrats' two-way vote share of the popular vote to mimic their two-way support in the polling data, and their percentage share of seats in the Congress. 

First, let's explore the trends over time in each cycle. Here, each point is a poll; it's size relfects the sample size and color represents the pollster. The dashed line represents the final two-way popular vote share of Democrats. A couple of observations from this are clear. We see that by election, some pollsters are systematically off. For example, the pink pollster in 2010 was consistently below the final election result, suggesting bias. Last, we see that there are trends in results over time. For example, in 2014 the polls got closer and closer to the true result over time. Further investigation shows that poll results are not normally distributed around the result **across time**, suggesting we will need a time-dependent model. 

<center>
![](figures/time_series.png)
![](figures/normal_dists.png)
</center>

It's also worth exploring the relationship between polls and two-way seats won. While I later improve upon this through modeling, a crude measure is the average poll result within 1 week of election day, weighted by sample size. The correlation between this and two-way seat share is 0.82 suggesting a strong positive relationship. 

<center>
![](figures/correlation.png)
</center>


##Estimating pollster and universe bias
To estimate bias for each pollster and universe, I use a Bayesian random-walk models anchored to the true final election results. For the first cycle a pollster/universe is used in, its prior is normally distributed around 0pp and assumed to be less than 20pp 95% of the time, in either direction. This prior is updated to be the posterior from the most recent previous cycle the pollster/universe was active in. Full specification of the theoretical model can be found in Appendix A; implementation specifications and key convergance diagnostics can be found in Appendix B. 

Below I plot the final bias estimate for each pollster. For example, for a pollster who polled in 2014 but not 2016, this will be their 2014 posterior results. Most pollsters are not biased by more than a percentage point in either direction. 'Gallup Low-Turnout' was the mostly conservative estimate (they took 4 polls in 1 election cycle). 'Diageo/Hotline' most consistently overestimted Democratic support (they took 7 polls in 2 election cycles). Bloomberg was the least biased pollseter with an average bias of -0.00008 across their 12 polls in 4 cycles. Full results can be found in Appendix B.

<center>
![](figures/pollster_bias.png)
</center>

Looking more closely at pollsters that were active in at least 5 of the 6 cycles examine, we some variation in bias across cycles. For example, CBS/NYT strongly oveestimated Democratic support in 2006, but became less and less biased each cycle. Others were too conservative in some cycles and too liberal in others. Fox News underestimated Democratic support in all.

<center>
![](figures/pollster_cycle.png)
</center>

Additionally, we see that most sampling universes also show some overestimation of Democratic support. Our posterior observation from the 2016 cycle shows that likely voter universes across pollsters were biased 1.7pp in favor of Democrats, registered voter universes were biased 2.4pp and samples of just adults were biased nearly 5pp in favor of Democrats. Full results can be found in Appendix B.

<center>
![](figures/univ_bias.png)
</center>

These trends were fairly stable over time. The rank order of the universes was the same for all elections except 2006. Both adult and registered voter universes were stable around their final estimate since the 2010 cycle. In 2010 and 2012, there was basically no bias in likely voter universes, but this increased in 2014 and 2016. 

<center>
![](figures/univ_cycle.png)
</center>

##Week-by-week estimates of support by cycle
Using the final estimates of bias for pollsters and universes as priors, I now refit the random-walk models, but with no anchor to the true result. This allows us to generate estimates week-by-week for each election, including a final estimate of election outcome, simulating a future prediction. The results are slightly overfit, especially for 2016, since the true results in each election updated the priors which are now inputs to the model. For 2016 specifically, the priors are derived from posterior distribution of the model anchored in the true result, so we should expect the model to be very precise. For full model specification see Appendix A and Appendix B for implementaton, code and full results. 

<center>
![](figures/time_series_with_trend.png)
</center>

<center>
![](figures/final_est_vs_seats.png)
</center>

##Conclusions
Using the estimate for the true current level of support, about 54%, and the parameter estimates from the regression model previously fit, I predict democrats will win about 52% of the seats, or 225 seats, with a 2.5% lower bound of 177 seats and a 97.5% upper bound of 273 seats. This estimate is similar to other's. For example, one respected [__author__](http://www.centerforpolitics.org/crystalball/articles/partisan-gerrymandering-and-the-outlook-for-the-2018-u-s-house-elections/?mc_cid=51acce7748&mc_eid=e5ff9cb0c5) finds an 8pp advantage in the generic ballot for Democrats will yield 224 Democratic seats. 

##Appendix A
To answer question 1 above, I follow [__Jackman (2005)__](http://eppsac.utdallas.edu/files/jackman/CAJP%2040-4%20Jackman.pdf) to specify my model to estimate biases, but with an added term for sampling universe. A given poll is assumed to be normally distruted with support as the mean and the standard deviation a function of $y_i$ and sample size. This would be specified as:
$$y_i \sim \mathcal{N}(\mu_i, \sigma^2_i)$$
That poll is centered around mean $\mu_i$, which itself is a function of $\alpha_t$, the true value of support at the time the poll was taken $t$, $\delta_j$, the bias of pollster $j$, and $\theta_k$, the bias of sampling universe $k$. Fully specified, this is: 
$$\mu_i = \alpha_{t_i} + \delta_{j_i} + \theta_{k_i}$$
Due to the trends we see in our initial data exploration, a random walk model is appropriate. In such a model, support at time $t$ is normally distributed around support at time $t - 1$. 
$$ \alpha_t \sim \mathcal{N}(\alpha_{t-1}, \omega^2) $$
By anchoring the model in the final election results, and by using a random walk, I will be able to estimate the consistent bias, $\delta$, of each pollster and the effect, $\theta$, of different sampling universes. 

For these given specifications, we have the following priors: 
$$ \sigma^2_i = \sqrt{\frac{y_i(1-y_i)}{n_i}},\ \ \ \delta_j \sim \mathcal{N}(0,1),\ \ \ \theta_k \sim \mathcal{N}(0,1),\ \ \ \alpha_1 \sim \mathcal{U}(0.46, 0.56),\ \ \ \omega^2 \sim IG(1/2,1/2)$$
$\sigma^2_i$ just follows the formula for standard deviation of a sample. For pollster biases ($\delta$), my prior is that there is no bias with a standard deviation large enough to capture 100% bias; my prior for bias from sampling universe ($\theta$) is the same. As a prior for the starting true value of support ($\alpha_1$), I use a uniform distribution over the minimum and maximum actual vote share of Democrats in the six elections analyzed. Lastly, as a prior for the true standard deviation of support ($\omega$), I use the inverse gamma distribution with an effective sample size of 1 and a prior guess of 1 like the standard deviation for $\delta$ and $\theta$.

To answer question 2 above, I will use the pollster and universe biases estimated above, and the same random walk algorithm to generate a final polling average at the time of the election, $\alpha_E$. I will then use the following model to estimate number of seats: 
$$ S_{cycle} \sim \mathcal{N}(\phi_{cycle}, \sigma^2) $$
$$\phi_{cycle} = \beta_0 + \beta_1*\alpha_{E_{cycle}},\ \ \ cycle = 2006,...,2016 $$
My priors for this model are: 
$$ \beta_0 \sim \mathcal{N}(0, 1),\ \ \ \beta_1 \sim \mathcal{N}(1,1),\ \ \ \sigma^2 \sim IG(1/2, 1/2)$$
$\beta_0$ here has a prior of 0 seats in the House of Representatives with a standard deviation 1. $\beta_1$ has a prior that says a 1 unit increase in $\alpha_{E_{cycle}}$ (a 100 perctange point increase in the Democrats' modeled vote share) is associated with a 100 percentage point increase in the share of seats awarded to Democrats, with a standard deviation of the same. Lastly, I use an inverse gamma distribution with a prior guess of 1 and effective sample size of 1 for the standard deviation. 

To answer question 3, I will use the same random walk algorithm already mentioned, along with the pollster and universe biases to generate a polling average for today. I will then use this $\alpha$ with the coefficients estimated in the second model to predict the number of seats Democrats will win in 2018. 

##Appendix B
###Functions and setup 
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(rjags)
library(cowplot)
set.seed(102)
scipen=999

data_prep <- function(data, res, year, anchor = T) { 
  data <- data %>% 
    filter(cycle == year) %>%
    mutate(pollster_num = as.numeric(as.factor(as.character(pollster))),
           univ_num = as.numeric(univ),
           prec = 1 / (sqrt((twoway * (1 - twoway)) / n_size)),
           week_adj = -1 * (week - max(week)) + 1) %>%
    select(-pollster_raw)
  
  data_jags = as.list(data)
  
  
  if(anchor) {
    xi <- rep(NA, max(length(unique(data$week_adj)), max(data$week_adj))+1)
    xi[max(data$week_adj+1)] <- res$twoway_vote[res$cycle == year]
  }  else { 
    xi <- rep(NA, max(length(unique(data$week_adj)), max(data$week_adj)))
    }
  
  data_jags$xi <- xi
  return(data_jags)
}

bias_priors <- function(data_jags, deltas, thetas) { 
  thetas <- thetas %>%
    filter(theta_univ %in% unique(data_jags$univ)) %>%
    mutate(theta_univ_num = as.numeric(theta_univ)) %>%
    arrange(theta_univ_num)
  
  deltas <- deltas %>%
    filter(delta_pollster %in% unique(data_jags$pollster)) %>%
    mutate(delta_pollster_num = as.numeric(as.factor(as.character(delta_pollster)))) %>%
    arrange(delta_pollster_num)
  
  data_jags <- append(data_jags, as.list(thetas))
  data_jags <- append(data_jags, as.list(deltas))
  return(data_jags)
}

run_model <- function(data_jags,
                      anchor = T,
                      chains = 4, 
                      thining = 10, 
                      burnin = 10000, 
                      iter = 1000000, 
                      params = c("xi", "delta", "theta")) { 
  mod_string_1 <- " model {
  xi[1] ~ dunif(0.46, 0.56) #The lower and upper limit of Dem two-way vote share in the 6 elections examined.

  for(i in 1:length(twoway)){
    mu[i] <- xi[week_adj[i]] + delta[pollster_num[i]] + theta[univ_num[i]]
    twoway[i] ~ dnorm(mu[i],prec[i])
  }

  for(t in 2:length(xi)){
    xi[t] ~ dnorm(xi[t-1],tau)
  }

  ## prior for standard deviations
  #omega2 ~ dgamma(1.0/2.0,1.0/2.0) I(0.001, 0.999)
  omega ~ dunif(0, .1)
  tau <- 1/pow(omega,2) "
  
  if(anchor) {
    mod_string_2 <- "
      ## priors for house effects
      for (i in 1:max(pollster_num)) {
        delta[i] ~ dnorm(delta_mu[i], 1.0/delta_sigma2[i])
      }

       for (i in 1:max(univ_num)) {
        theta[i] ~ dnorm(theta_mu[i], 1.0/theta_sigma2[i])
      }
      } "
  } else {
    mod_string_2 <- "
      ## priors for house effects
      for (i in 1:max(pollster_num)) {
        delta[i] = delta_mu[i]
      }

       for (i in 1:max(univ_num)) {
        theta[i] = theta_mu[i]
      }
      } "
  }
  
  mod_string <- paste(mod_string_1, mod_string_2)

  mod <- jags.model(textConnection(mod_string), data = data_jags, n.chains = chains)
  update(mod, burnin) # burn-in
  mod_sim <- coda.samples(model = mod, variable.names = params, n.iter= iter, thin = thining)
  return(mod_sim)
}

calculate_priors <- function(mod_res, year, data_jags) { 
  mod_csim <- as.mcmc(do.call(rbind, mod_res))
  param_ests <- data.frame(iter_mean = colMeans(mod_csim),
                           iter_sigma2 = (apply(mod_csim, 2, FUN ="sd"))^2)
  
  delta_est <- param_ests %>% 
    filter(substr(row.names(param_ests),1,1) == 'd') %>%
    mutate(delta_pollster_num = data_jags$delta_pollster_num,
           pollster = data_jags$delta_pollster) %>%
    full_join(data.frame(pollster = levels(data_jags$pollster)), by = "pollster") %>%
    mutate(delta_cycle = year,
           delta_mu = iter_mean,
           delta_sigma2 = iter_sigma2,
           delta_pollster = pollster) %>%
    select(delta_cycle, delta_pollster, delta_mu, delta_sigma2)

  theta_est <- param_ests %>% 
    filter(substr(row.names(param_ests),1,1) == 't') %>%
    mutate(theta_univ_num = data_jags$theta_univ_num,
           univ = data_jags$theta_univ) %>%
    full_join(data.frame(univ = levels(data_jags$univ)), by = "univ") %>%
    mutate(theta_cycle = year,
           theta_mu = iter_mean,
           theta_sigma2 = iter_sigma2,
           theta_univ = univ) %>%
    select(theta_cycle, theta_univ, theta_mu, theta_sigma2)
  
  return(list(deltas_est = delta_est, thetas_est = theta_est))
}

update_priors <- function(deltas_all, thetas_all, deltas_new, thetas_new) {
  x <- rbind(deltas_all, deltas_new)
  y <- rbind(thetas_all, thetas_new)

  w <- x %>%
    filter(!is.na(delta_mu)) %>%
    group_by(delta_pollster) %>%
    filter(delta_cycle == max(delta_cycle)) %>%
    ungroup()

  z <- y %>%
    filter(!is.na(theta_mu)) %>%
    group_by(theta_univ) %>%
    filter(theta_cycle == max(theta_cycle)) %>%
    ungroup()
  
  return(list(deltas = w, deltas_all = x, thetas = z, thetas_all = y))
}

convergence_diagnostics <- function(chains = 4, 
                                    thining = 10, 
                                    burnin = 10000, 
                                    iter = 1000000, 
                                    data_jags) { 
  xi <- paste0("xi[", sample(seq(1,length(data_jags$xi)), 1), "]")
  delta <- paste0("delta[", sample(seq(1,length(data_jags$delta_pollster)), 1), "]")
  theta <- paste0("theta[", sample(seq(1,length(data_jags$theta_univ)), 1), "]")
  params <- c(xi, delta, theta)
  
  mod_res <- run_model(chains = chains, 
                       thining = thining, 
                       burnin = burnin, 
                       iter = iter, 
                       params = params,
                       data_jags = data_jags)
  
  return(list(gelman = gelman.diag(mod_res), autocorr = autocorr.diag(mod_res)))
}

extract_time_est <- function(mod_res, year, data_jags) { 
  mod_csim <- as.mcmc(do.call(rbind, mod_res))
  param_ests <- data.frame(iter_mean = colMeans(mod_csim),
                           iter_sigma2 = (apply(mod_csim, 2, FUN ="sd"))^2)
  
  time_est <- param_ests %>% 
    filter(substr(row.names(param_ests),1,1) == 'x') %>%
    mutate(time_before_elec = seq((length(data_jags$xi) - 1), 0, -1),
           upper_bound = iter_mean + 1.96*sqrt(iter_sigma2),
           lower_bound = iter_mean - 1.96*sqrt(iter_sigma2),
           cycle = year)

  return(time_est)
}
```

###Load, prep and explore data
```{r}
pollster_lkup <- read.csv("pollsters.csv")

res <- read.csv("election_results.csv") %>%
  mutate(twoway_vote = dem_vote/(dem_vote+rep_vote),
         twoway_seat = dem_seats/(dem_seats+rep_seats)) %>%
  arrange(cycle)

polls <- read.csv("training_dat.csv") %>%
  mutate(twoway = dem/(dem+rep)) %>% 
  inner_join(res[,c("cycle","date")], by="cycle") %>%
  mutate(week = round(as.numeric((as.Date(as.character(date),  format="%m/%d/%y") - 
           as.Date(as.character(end_date),  format="%m/%d/%y")) + 
           (as.Date(as.character(end_date),  format="%m/%d/%y") - 
           as.Date(as.character(start_date),  format="%m/%d/%y"))/2)/7),
         n_size = as.numeric(as.character(n_size)))

polling_summary <- polls %>% 
  group_by(pollster) %>%
  summarise(`Total N-Size` = sum(n_size), 
            `# of Polls` = n(), 
            `# of Cycles` = length(unique(cycle))) %>%
  arrange(desc(`Total N-Size`)) %>%
  inner_join(pollster_lkup, by = "pollster") %>%
    mutate(pollster_raw = factor(pollster_raw, levels = pollster_raw[order(`Total N-Size`)]))

print(polling_summary) #Flextable
```

```{r, include=FALSE}
#Plots for above
x1 <- ggplot(data = (polling_summary[1:5,]), aes(y = `# of Cycles`, x = pollster_raw)) +
  geom_bar(stat = 'identity') +
  coord_flip() + 
  theme_bw() + 
  xlab("") 

x2 <- ggplot(data = (polling_summary[1:5,]), aes(y = `Total N-Size`, x = pollster_raw)) +
  geom_bar(stat = 'identity') +
  coord_flip() + 
  theme_bw() + 
  xlab("") + 
  theme(axis.text.y = element_blank())

x3 <- ggplot(data = (polling_summary[1:5,]), aes(y = `# of Polls`, x = pollster_raw)) +
  geom_bar(stat = 'identity') +
  coord_flip() + 
  theme_bw() + 
  xlab("") + 
  theme(axis.text.y = element_blank())

ggsave(filename = "figures/data_breakdown.png", plot = plot_grid(x1, x2, x3, nrow = 1), width = 8, height = 4, units = "in")

timeseries <- ggplot(data=polls, aes(x=week, y=twoway, size=sqrt(n_size), color = pollster)) + 
  geom_point(alpha=0.3) +
  theme_bw() + 
  facet_wrap(~cycle) +
  scale_x_reverse(name = "Weeks Before Election") +
  scale_y_continuous(name = "Democratic Two-way Vote Share", labels=scales::percent) + 
  geom_hline(data=res, aes(yintercept = twoway_vote), linetype="dashed") +
  guides(size=F, color = F)

ggsave(filename = "figures/time_series.png", plot = timeseries, width = 8, height = 4, units = "in")

normal_dists <- ggplot(data=polls, aes(x=round(twoway,2))) + 
  geom_histogram() +
  theme_bw() + 
  facet_wrap(~cycle) +
  scale_x_continuous(name = "Democratic Two-way Vote Share", labels=scales::percent) + 
  ylab("Number of Polls") +
  geom_vline(data=res, aes(xintercept = twoway_vote), linetype="dashed") 

ggsave(filename = "figures/normal_dists.png", plot = normal_dists, width = 8, height = 4, units = "in")

avgs <- polls %>%
  filter(week < 2) %>%
  group_by(cycle) %>%
  summarise(avg = sum(n_size*twoway)/sum(n_size)) %>%
  inner_join(res,by="cycle")

cor(avgs$avg, avgs$twoway_seat)

correlation <- ggplot(avgs, aes(x=avg, y=twoway_seat)) + 
  geom_point() +
  theme_bw() +
  scale_y_continuous(name = "Dems' Share of Seats Won", labels = scales::percent, limits = c(0.4,0.6)) +
  scale_x_continuous(name = "Dems' Avg Final Poll Results", labels = scales::percent, limits = c(0.4,0.6)) +
  stat_smooth(method = "lm", se=F) +
  theme(axis.title.x = element_text(size=8),
        axis.title.y = element_text(size=8))

ggsave(filename = "figures/correlation.png", plot = correlation, width = 3, height = 3, units = "in")
```

###Estimate bias for pollsters and universes
```{r, warning=FALSE, message=FALSE, results=FALSE}
deltas <- data.frame(delta_cycle = 0,
                     delta_pollster = unique(polls$pollster),
                     delta_mu = rep(0, length(unique(polls$pollster))), 
                     delta_sigma2 = rep(0.2, length(unique(polls$pollster))))
deltas_all <- deltas

thetas <- data.frame(theta_cycle = 0,
                     theta_univ = unique(polls$univ),
                     theta_mu = rep(0, length(unique(polls$univ))), 
                     theta_sigma2 = rep(0.2, length(unique(polls$univ))))
thetas_all <- thetas

convergence <- list()

#Estimation
for(cycle in res$cycle) {
  data_jags <- data_prep(data = polls, res = res, year = cycle)
  data_jags <- bias_priors(data_jags = data_jags, deltas = deltas, thetas = thetas)
  convergence[[paste(cycle)]] <- convergence_diagnostics(data_jags = data_jags)
  mod_res <- run_model(data_jags = data_jags)

  prior_ests <- calculate_priors(mod_res = mod_res, year = cycle, data_jags = data_jags)
  new_priors <- update_priors(deltas_all = deltas_all, thetas_all = thetas_all, 
                              deltas_new = prior_ests$deltas_est, thetas_new = prior_ests$thetas_est)
  deltas <- new_priors$deltas 
  deltas_all <- new_priors$deltas_all 

  thetas <- new_priors$thetas 
  thetas_all <- new_priors$thetas_all
}

deltas <- deltas %>% 
    arrange(delta_mu) %>%
    inner_join(pollster_lkup, by = c("delta_pollster" = "pollster")) %>%
    mutate(pollster_raw = factor(pollster_raw, levels = pollster_raw[order(delta_mu)]))

deltas_all <- deltas_all %>%
    inner_join(pollster_lkup, by = c("delta_pollster" = "pollster"))

thetas <- thetas %>% 
    arrange(theta_mu) %>%
    mutate(theta_univ = factor(theta_univ, levels = theta_univ[order(theta_mu)]))
```

```{r, echo=FALSE}
for(cycle in res$cycle){ 
  message(paste0("Sample convergance diagnostics for ", cycle, " parameters:"))
  print(convergence[[paste(cycle)]]$gelman)
  print(convergence[[paste(cycle)]]$autocorr)
  }

message("Final estimates of pollster bias:")
print(deltas) #Flextable

message("Estimate for each pollster and cycle:")
print(deltas_all) #Flextable

message("Final estimates of sampling universe bias:")
print(thetas) #Flextable

message("Estimate for each universe and cycle:")
print(thetas_all) #Flextable
```

```{r, include = FALSE}
pollster_bias <- ggplot(deltas, aes(x=pollster_raw, y=delta_mu)) + 
  geom_point(aes(color = delta_mu)) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_color_continuous(low="red", high="blue") +
  theme_bw() +
  guides(color = F) +
  xlab("Pollster") +
  scale_y_continuous(name = "Bias Estimate", labels = scales::percent, limits = c(-0.055,0.055)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, vjust = 0.5))

ggsave(filename = "figures/pollster_bias.png", plot = pollster_bias, width = 8, height = 4, units = "in")

temp <- deltas_all %>%
  mutate(delta_cycle = ifelse(delta_cycle == 0, 2004, delta_cycle))

pollsters <- temp %>% 
  filter(!is.na(delta_mu)) %>% 
  group_by(delta_pollster) %>% 
  summarise(n = n()) %>% 
  filter(n > 5) 

temp <- temp %>%
  filter(delta_pollster %in% pollsters$delta_pollster, !is.na(delta_mu))

pollster_cycle <- ggplot(temp, aes(x=delta_cycle, y=delta_mu, group = pollster_raw)) + 
  geom_point(alpha = 0.9, aes(color = pollster_raw)) +
  geom_line(alpha = 0.8, color = "lightgrey") +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_x_continuous(breaks = c(2004, 2006, 2008, 2010, 2012, 2014, 2016), 
                   labels = c("", "2006", "2008", "2010", "2012", "2014", "2016")) +
  theme_bw() +
  xlab("Cycle") +
  scale_y_continuous(name = "Bias Estimate", labels = scales::percent, limits = c(-0.05,0.05)) + 
  scale_color_discrete(name = "Pollster")

ggsave(filename = "figures/pollster_cycle.png", plot = pollster_cycle, width = 8, height = 4, units = "in")

univ_bias <- ggplot(thetas, aes(x=theta_univ, y=theta_mu)) + 
  geom_point() +
  theme_bw() +
  guides(color = F) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  xlab("Sampling Universe") +
  scale_y_continuous(name = "Bias Estimate", labels = scales::percent, limits = c(0,0.05)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10, vjust = 0.5))

ggsave(filename = "figures/univ_bias.png", plot = univ_bias, width = 4, height = 2, units = "in")

temp <- thetas_all %>%
  mutate(theta_cycle = ifelse(theta_cycle == 0, 2004, theta_cycle)) %>%
  filter(!is.na(theta_mu))

univ_cycle <- ggplot(temp, aes(x=theta_cycle, y=theta_mu, group = theta_univ)) + 
  geom_point(alpha = 0.9, aes(color = theta_univ)) +
  geom_line(alpha = 0.8, color = "lightgrey") +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_x_continuous(breaks = c(2004, 2006, 2008, 2010, 2012, 2014, 2016), 
                   labels = c("", "2006", "2008", "2010", "2012", "2014", "2016")) +
  theme_bw() +
  xlab("Cycle") +
  scale_y_continuous(name = "Bias Estimate", labels = scales::percent, limits = c(-0.05,0.05)) + 
  scale_color_discrete(name = "Sampling\nUnivese")

ggsave(filename = "figures/univ_cycle.png", plot = univ_cycle, width = 8, height = 4, units = "in")
```

```{r, warning=FALSE, message=FALSE, results=FALSE}
#Estimate week-by-week movement using past pollster and universe bias
all_cycle_est <- data.frame(iter_mean = numeric(0),
                            iter_sigma2 = numeric(0),
                            time_before_elec = numeric(0),
                            upper_bound = numeric(0),
                            lower_bound = numeric(0),
                            cycle = numeric(0))

for(cycle in res$cycle) {
  data_jags <- data_prep(data = polls, res = res, year = cycle, anchor = F)
  data_jags <- bias_priors(data_jags = data_jags, deltas = deltas, thetas = thetas)

  mod_res <- run_model(data_jags = data_jags, params = c("xi"), anchor = F)
  cycle_time_est <- extract_time_est(mod_res = mod_res, year = cycle, data_jags = data_jags)
  all_cycle_est <- rbind(all_cycle_est, cycle_time_est)
}
```

```{r, include=FALSE}
time_series_with_trend <- ggplot(data=all_cycle_est, aes(x=time_before_elec, y=iter_mean)) + 
  geom_point(data=polls, aes(x=week, y=twoway, size=sqrt(n_size)), alpha=0.2) +
  geom_ribbon(aes(ymin=lower_bound,ymax=upper_bound), alpha = 0.5) +
  geom_hline(data=res, aes(yintercept = twoway_vote), linetype="dashed") +
  geom_line(color = "blue") +
  theme_bw() + 
  facet_wrap(~cycle) +
  scale_x_reverse(name = "Weeks Before Election") +
  scale_y_continuous(name = "Democratic Two-way Vote Share", labels=scales::percent) + 
  guides(size=F, color = F)

ggsave(filename = "figures/time_series_with_trend.png", plot = time_series_with_trend, width = 8, height = 4, units = "in")

final_ests <- all_cycle_est %>% 
  filter(time_before_elec == 0) %>%
  inner_join(avgs, by = "cycle") 

lm_fit <- ggplot(final_ests, aes(x = twoway_seat, y = iter_mean)) + 
  geom_point() + 
  geom_errorbar(aes(ymin=lower_bound, ymax=upper_bound)) +
  coord_flip() + 
  theme_bw() + 
  scale_y_continuous(name = "Election Forecast", limits = c(0.4, 0.65), labels = scales::percent) + 
  scale_x_continuous(name = "Observed Seats Won", limits = c(0.4, 0.6), labels = scales::percent) + 
  stat_smooth(method = "lm") 

ggsave(filename = "figures/final_est_vs_seats.png", plot = lm_fit, width = 3, height = 3, units = "in")

tbl <- final_ests %>%
  mutate(`Forecast` = scales::percent(round(iter_mean,3)),
         `Popular Vote` = scales::percent(round(twoway_vote,3)), 
         `Seat Share` = scales::percent(round(twoway_seat,3)),
         `Forecast Error` = scales::percent(round(twoway_vote - iter_mean,3)),
         Cycle = cycle) %>%
  select(c(Cycle, `Forecast`, `Popular Vote`, `Seat Share`, `Forecast Error`))
```

```{r}
summary(glm(twoway_seat ~ iter_mean, data = all_cycle_est %>% 
  filter(time_before_elec == 0) %>%
  inner_join(avgs, by = "cycle")))
```

