## step 1, initialize
mu_out = numeric(n_iter)
accpt = 0
mu_now = mu_init
lg_now = lg(mu=mu_now, n=n, ybar=ybar)
## step 2, iterate
for (i in 1:n_iter) {
## step 2a
mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
## step 2b
lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
lalpha = lg_cand - lg_now # log of acceptance ratio
alpha = exp(lalpha)
## step 2c
u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
if (u < alpha) { # then accept the candidate
mu_now = mu_cand
accpt = accpt + 1 # to keep track of acceptance
lg_now = lg_cand
}
## collect results
mu_out[i] = mu_now # save this iteration's value of mu
}
## return a list of output
list(mu=mu_out, accpt=accpt/n_iter)
}
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=4.0)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=1.5)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.5)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=1.5)
mean(post$mu)
setwd("~/src/BSTM/week4")
dat = read.csv(file="pctgrowth.csv", header=TRUE)
means_anova = tapply(dat$y, INDEX=dat$grp, FUN=mean)
means_anova
View(dat)
library("rjags")
mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:max(grp)) {
mu[j] ~ dnorm(mu_pri, prec_pri)
}
prec ~ dgamma(2.0/2.0, 2.0*1.0/2.0)
mu_pri ~ dnorm(0, 1/1e6)
prec_pri ~ dgamma(1.0/2.0, 1.0*3.0/2.0)
sig = sqrt( 1.0 / prec)
tao_sq = sqrt( 1.0 / prec_pri)
} "
set.seed(113)
data_jags = as.list(dat)
data_jags
params = c("tao_sq", "mu", "sig")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
plot(mod_sim, ask = T)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)
(pm_params = colMeans(mod_csim))
means_theta = pm_params[1:5]
means_theta
means_anova
plot(means_anova)
points(means_theta, col="red")
sd(means_theta)
sd(means_anova)
mean(means_anova)
mean(means_theta)
library("MASS")
data("OME")
dat = subset(OME, OME != "N/A")
dat$OME = factor(dat$OME) # relabel OME
dat$ID = as.numeric(factor(dat$ID)) # relabel ID so there are no gaps in numbers (they now go from 1 to 63)
mod_glm = glm(Correct/Trials ~ Age + OME + Loud + Noise, data=dat, weights=Trials, family="binomial")
X = model.matrix(mod_glm)[,-1]
mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dbin(phi[i], n[i])
logit(phi[i]) = b0 + b[1]*Age[i] + b[2]*OMElow[i] + b[3]*Loud[i] + b[4]*Noiseincoherent[i]
}
b0 ~ dnorm(0.0, 1.0/5.0^2)
for (j in 1:4) {
b[j] ~ dnorm(0.0, 1.0/4.0^2)
}
} "
data_jags = as.list(as.data.frame(X))
data_jags$y = dat$Correct
data_jags$n = dat$Trials
data_jags$ID = dat$ID
data_jags
params = c("b0", "b")
mod1 = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1,
variable.names=params,
n.iter=5e3)
mod1_csim = as.mcmc(do.call(rbind, mod1_sim))
dic1 = dic.samples(mod1, n.iter=1e3)
mod2_string = " model {
for (i in 1:length(y)) {
y[i] ~ dbin(phi[i], n[i])
logit(phi[i]) = a[ID[i]] + b[1]*Age[i] + b[2]*OMElow[i] + b[3]*Loud[i] + b[4]*Noiseincoherent[i]
}
for (j in 1:max(ID)) {
a[j] ~ dnorm(a0, prec_a)
}
a0 ~ dnorm(0.0, 1.0/10.0^2)
prec_a ~ dgamma(1/2.0, 1.0/2.0)
tau = sqrt( 1.0 / prec_a )
for (j in 1:4) {
b[j] ~ dnorm(0.0, 1.0/4.0^2)
}
} "
params2 = c("a", "tau", "b")
mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
update(mod2, 1e3)
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))
dic2 = dic.samples(mod2, n.iter=1e3)
plot(mod2_sim)
gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)
effectiveSize(mod2_sim)
dic1
dic2
data("warpbreaks")
?warpbreaks
head(warpbreaks)
table(warpbreaks$wool, warpbreaks$tension)
boxplot(breaks ~ wool + tension, data=warpbreaks)
boxplot(log(breaks) ~ wool + tension, data=warpbreaks)
library("rjags")
mod1_string = " model {
for( i in 1:length(y)) {
y[i] ~ dnorm(mu[tensGrp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5/2.0, 5*2.0/2.0)
sig = sqrt(1.0 / prec)
} "
set.seed(83)
str(warpbreaks)
data1_jags = list(y=log(warpbreaks$breaks), tensGrp=as.numeric(warpbreaks$tension))
params1 = c("mu", "sig")
mod1 = jags.model(textConnection(mod1_string), data=data1_jags, n.chains=3)
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1,
variable.names=params1,
n.iter=5e3)
## convergence diagnostics
plot(mod1_sim)
gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
effectiveSize(mod1_sim)
summary(mod1_sim)
dic1 = dic.samples(mod1, n.iter=1e3)
dic1
X = model.matrix( ~ wool + tension, data=warpbreaks)
head(X)
tail(X)
mod2_string = " model {
for( i in 1:length(y)) {
y[i] ~ dnorm(mu[i], prec)
mu[i] = int + alpha*isWoolB[i] + beta[1]*isTensionM[i] + beta[2]*isTensionH[i]
}
int ~ dnorm(0.0, 1.0/1.0e6)
alpha ~ dnorm(0.0, 1.0/1.0e6)
for (j in 1:2) {
beta[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(3/2.0, 3*1.0/2.0)
sig = sqrt(1.0 / prec)
} "
data2_jags = list(y=log(warpbreaks$breaks), isWoolB=X[,"woolB"], isTensionM=X[,"tensionM"], isTensionH=X[,"tensionH"])
params2 = c("int", "alpha", "beta", "sig")
mod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)
update(mod2, 1e3)
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
## convergene diagnostics
plot(mod2_sim)
gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
effectiveSize(mod1_sim)
gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)
effectiveSize(mod2_sim)
summary(mod2_sim)
(dic2 = dic.samples(mod2, n.iter=1e3))
mod3_string = " model {
for( i in 1:length(y)) {
y[i] ~ dnorm(mu[woolGrp[i], tensGrp[i]], prec)
}
for (j in 1:max(woolGrp)) {
for (k in 1:max(tensGrp)) {
mu[j,k] ~ dnorm(0.0, 1.0/1.0e6)
}
}
prec ~ dgamma(3/2.0, 3*1.0/2.0)
sig = sqrt(1.0 / prec)
} "
str(warpbreaks)
data3_jags = list(y=log(warpbreaks$breaks), woolGrp=as.numeric(warpbreaks$wool), tensGrp=as.numeric(warpbreaks$tension))
params3 = c("mu", "sig")
mod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)
update(mod3, 1e3)
mod3_sim = coda.samples(model=mod3,
variable.names=params3,
n.iter=5e3)
mod3_csim = as.mcmc(do.call(rbind, mod3_sim))
plot(mod3_sim, ask=TRUE)
## convergence diagnostics
gelman.diag(mod3_sim)
autocorr.diag(mod3_sim)
effectiveSize(mod3_sim)
raftery.diag(mod3_sim)
(dic3 = dic.samples(mod3, n.iter=1e3))
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in
dic.samples(mod1, n.iter=1e3)
dic1 = dic.samples(mod1, n.iter=1e3)
?dgamma
?ddexp
#--laplace model
Xc = scale(Anscombe, center=TRUE, scale=TRUE)
str(Xc)
mod2_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
for (i in 1:3) {
b[i] ~ ddexp(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0, 1.0)
sig = sqrt(1.0 / prec)
} "
data2_jags = as.list(data.frame(Xc))
params2 = c("b", "sig")
inits2 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
data("Anscombe")
library("car")
data("Anscombe")
head(Anscombe)
?Anscombe
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in
dic1 = dic.samples(mod1, n.iter=1e3)
Xc = scale(Anscombe, center=TRUE, scale=TRUE)
str(Xc)
dic1
mod2_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
for (i in 1:3) {
b[i] ~ ddexp(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0, 1.0)
sig = sqrt(1.0 / prec)
} "
data2_jags = as.list(data.frame(Xc))
params2 = c("b", "sig")
inits2 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)
update(mod2, 1000) # burn-in
dic2 = dic.samples(mod2, n.iter=1e3)
dic2
dic1
dic2
mod1_sim = coda.samples(model=mod1,
variable.names=params3,
n.iter=5e3)
summary(mod1_sim)
#--orig model
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in
mod1_sim = coda.samples(model=mod1,
variable.names=params1,
n.iter=5e3)
summary(mod1_sim)
dic1 = dic.samples(mod1, n.iter=1e3)
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
summary(mod2_sim)
plot(mod2_sim)
plot(mod2_sim)
plot(mod2_sim)
plot(mod1_sim)
#--laplace model
Xc = scale(Anscombe, center=TRUE, scale=TRUE)
str(Xc)
mod2_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
for (i in 1:3) {
b[i] ~ ddexp(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0/2.0)
sig = sqrt(1.0 / prec)
} "
data2_jags = as.list(data.frame(Xc))
params2 = c("b", "sig")
inits2 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)
update(mod2, 1000) # burn-in
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
summary(mod2_sim)
dic2 = dic.samples(mod2, n.iter=1e3)
dic2
mod4_string = " model {
for( i in 1:length(y)) {
y[i] ~ dnorm(mu[woolGrp[i], tensGrp[i]], prec[woolGrp[i], tensGrp[i]])
}
for (j in 1:max(woolGrp)) {
for (k in 1:max(tensGrp)) {
mu[j,k] ~ dnorm(0.0, 1.0/1.0e6)
prec[j,k] ~ dgamma(1.0/2.0, 1.0/2.0)
sig[j,k] = sqrt(1.0 / prec[j,k])
}
}
} "
data4_jags = list(y=log(warpbreaks$breaks), woolGrp=as.numeric(warpbreaks$wool), tensGrp=as.numeric(warpbreaks$tension))
params4 = c("mu", "sig")
mod4 = jags.model(textConnection(mod4_string), data=data4_jags, n.chains=3)
update(mod4, 1e3)
mod4_sim = coda.samples(model=mod4,
variable.names=params4,
n.iter=5e3)
mod4_csim = as.mcmc(do.call(rbind, mod4_sim))
plot(mod4_sim, ask=TRUE)
gelman.diag(mod4_sim)
autocorr.diag(mod4_sim)
effectiveSize(mod4_sim)
raftery.diag(mod4_sim)
(dic4 = dic.samples(mod4, n.iter=1e3))
dat = read.csv(file="callers.csv", header=TRUE)
boxplot(calls ~ isgroup2, data=dat)
View(dat)
mod_string = " model {
for (i in 1:length(calls)) {
calls[i] ~ dpois( days_active[i] * lam[i] )
log(lam[i]) = b0 + b[1]*age[i] + b[2]*isgroup2[i]
}
b0 ~ dnorm(0.0, 1.0/10.0^2)
for (j in 1:2) {
b[j] ~ dnorm(0.0, 1.0/10.0^2)
}
} "
data_jags = as.list(dat)
params = c("b0", "b")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=10e4)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
## convergence diagnostics
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
## Analysis
summary(mod_sim)
head(modcsim)
head(mod_csim)
names(mod_csim)
mod_csim[,3]
head(mod_csim)
loglam <- mod_csim[,3] + mod_csim[,1]*29 + mod_csim[,2]*1
loglam
lam <- exp(loglam)
calls <- 30 * lam
hist(calls)
hist(calls)
hist(calls)
mean(calls >= 3)
loglam <- mod_csim[,3] + mod_csim[,1]*29 + mod_csim[,2]*0
lam <- exp(loglam)
calls <- 30 * lam
mean(calls >= 3)
hist(calls)
hist(calls)
loglam <- mod_csim[,3] + mod_csim[,1]*29 + mod_csim[,2]*1
lam <- exp(loglam)
calls <- 30 * lam
hist(calls)
mod_csim[,2]
mod_csim[,1]
mod_csim[,1]*29
dat = read.csv(file="callers.csv", header=TRUE)
data_jags = as.list(dat)
data_jags
params = c("b0", "b")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=10e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
summary(mod_sim)
x1 = (29,1,1)
x1 = c(29,1,1)
loglam <- mod_csim %*% x1
lam <- exp(loglam)
calls <- 30 * lam
mean(calls >= 3)
hist(calls)
