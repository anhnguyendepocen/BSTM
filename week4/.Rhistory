accpt = 0
mu_now = mu_init
lg_now = lg(mu=mu_now, n=n, ybar=ybar)
## step 2, iterate
for (i in 1:n_iter) {
## step 2a
mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
## step 2b
lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
lalpha = lg_cand - lg_now # log of acceptance ratio
alpha = exp(lalpha)
## step 2c
u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
if (u < alpha) { # then accept the candidate
mu_now = mu_cand
accpt = accpt + 1 # to keep track of acceptance
lg_now = lg_cand
}
## collect results
mu_out[i] = mu_now # save this iteration's value of mu
}
## return a list of output
list(mu=mu_out, accpt=accpt/n_iter)
}
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=4.0)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=1.5)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.5)
str(post)
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=1.5)
mean(post$mu)
library("car")  # load the 'car' package
data("Anscombe")  # load the data set
?Anscombe  # read a description of the data
head(Anscombe)  # look at the first few lines of the data
install.packages("car")
library("car")  # load the 'car' package
data("Anscombe")  # load the data set
?Anscombe  # read a description of the data
head(Anscombe)  # look at the first few lines of the data
pairs(Anscombe)  # scatter plots for each pair of variables
summary(lm(education ~ ., data=Anscombe))
library("rjags")
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod1_string), data=data1_jags, inits=inits1, n.chains=3)
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in
mod1_sim = coda.samples(model=mod1,
variable.names=params1,
n.iter=5000)
mod1_csim = do.call(rbind, mod1_sim) # combine multiple chains
plot(mod1_sim)
gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
plot(lm(education ~ ., data=Anscombe))
mod_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits1 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in
dic.samples(mod1, n.iter=1e3)
#Q4
mod2_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:2) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits2 = function() {
inits = list("b"=rnorm(2,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod2 = jags.model(textConnection(mod2_string), data=data_jags, inits=inits1, n.chains=3)
update(mod2, 1000) # burn-in
dic.samples(mod2, n.iter=1e3)
mod2 = jags.model(textConnection(mod2_string), data=data_jags, inits=inits2, n.chains=3)
update(mod2, 1000) # burn-in
dic.samples(mod2, n.iter=1e3)
mod3_string = " model {
for (i in 1:length(education)) {
education[i] ~ dnorm(mu[i], prec)
mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*income[i]*young[i]
}
b0 ~ dnorm(0.0, 1.0/1.0e6)
for (i in 1:3) {
b[i] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
## Initial guess of variance based on overall
## variance of education variable. Uses low prior
## effective sample size. Technically, this is not
## a true 'prior', but it is not very informative.
sig2 = 1.0 / prec
sig = sqrt(sig2)
} "
data_jags = as.list(Anscombe)
params1 = c("b", "sig")
inits3 = function() {
inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod3 = jags.model(textConnection(mod3_string), data=data_jags, inits=inits3, n.chains=3)
update(mod3, 1000) # burn-in
dic.samples(mod3, n.iter=1e3)
(pm_params1 = colMeans(mod1_csim)) # posterior mean
mod1_sim = coda.samples(model=mod1,
variable.names=params1,
n.iter=5000)
mod1_csim = do.call(rbind, mod1_sim) # combine multiple chains
(pm_params1 = colMeans(mod1_csim)) # posterior mean
mod1_csim
mean(mod1_csim$b[1] > 0)
mean(mod1_csim[,1] > 0)
summary(mod1_sim)
b1 = rnorm(10000, 0.08098, 0.008664)
mean(b1 > 0.0)
dic.samples(mod1, n.iter=1e3)
data("PlantGrowth")
library("rjags")
mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec[grp[i]])
}
for (j in 1:3) {
mu[j] ~ dnorm(0.0, 1.0/1.0e6)
prec[j] ~ dgamma(5/2.0, 5*1.0/2.0)
sig[j] = sqrt( 1.0 / prec[j])
}
} "
set.seed(82)
str(PlantGrowth)
data_jags = list(y=PlantGrowth$weight,
grp=as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(3,1.0,1.0))
}
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim)) # combined chains
summary(mod_sim)
dic.samples(mod, n.iter=1e3)
dic2 = dic.samples(mod, n.iter=1e3)
orig_mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2.0)
sig = sqrt( 1.0 / prec )
} "
orig_inits = function() {
inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
mod = jags.model(textConnection(orig_mod_string), data=data_jags, inits=orig_inits, n.chains=3)
orig_mod = jags.model(textConnection(orig_mod_string), data=data_jags, inits=orig_inits, n.chains=3)
mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec[grp[i]])
}
for (j in 1:3) {
mu[j] ~ dnorm(0.0, 1.0/1.0e6)
prec[j] ~ dgamma(5/2.0, 5*1.0/2.0)
sig[j] = sqrt( 1.0 / prec[j])
}
} "
set.seed(82)
str(PlantGrowth)
data_jags = list(y=PlantGrowth$weight,
grp=as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(3,1.0,1.0))
}
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim)) # combined chains
dic1 = dic.samples(orig_mod, n.iter=1e3)
dic1 - dic2
dic1
dic2
5.9605551-4.5848092
5.0669822-5.4782671
update(orig_mod, 1e3)
orig_mod_sim = coda.samples(model=orig_mod,
variable.names=params,
n.iter=5e3)
orig_mod_csim = as.mcmc(do.call(rbind, orig_mod_sim)) # combined chains
head(orig_mod_csim)
orig_mod_csim[3]-orig_mod_csim[1]
orig_mod_csim[,3]-orig_mod_csim[,1]
HPDinterval(orig_mod_csim[,3]-orig_mod_csim[,1])
library("MASS")
data("OME")
?OME # background on the data
head(OME)
any(is.na(OME)) # check for missing values
dat = subset(OME, OME != "N/A") # manually remove OME missing values identified with "N/A"
dat$OME = factor(dat$OME)
str(dat)
plot(dat$Age, dat$Correct / dat$Trials )
plot(dat$OME, dat$Correct / dat$Trials )
plot(dat$Loud, dat$Correct / dat$Trials )
plot(dat$Noise, dat$Correct / dat$Trials )
mod_glm = glm(Correct/Trials ~ Age + OME + Loud + Noise, data=dat, weights=Trials, family="binomial")
summary(mod_glm)
plot(residuals(mod_glm, type="deviance"))
plot(fitted(mod_glm), dat$Correct/dat$Trials)
X = model.matrix(mod_glm)[,-1] # -1 removes the column of 1s for the intercept
head(X)
mod_string = " model {
for (i in 1:length(y)) {
y[i] ~ dbin(phi[i], n[i])
logit(phi[i]) = b0 + b[1]*Age[i] + b[2]*OMElow[i] + b[3]*Loud[i] + b[4]*Noiseincoherent[i]
}
b0 ~ dnorm(0.0, 1.0/5.0^2)
for (j in 1:4) {
b[j] ~ dnorm(0.0, 1.0/4.0^2)
}
} "
data_jags = as.list(as.data.frame(X))
data_jags$y = dat$Correct # this will not work if there are missing values in dat (because they would be ignored by model.matrix). Always make sure that the data are accurately pre-processed for JAGS.
data_jags$n = dat$Trials
str(data_jags) # make sure that all variables have the same number of observations (712).
params = c("int", "b")
mod1 = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1,
variable.names=params,
n.iter=5e3)
params = c("b0", "b")
mod1 = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1,
variable.names=params,
n.iter=5e3)
mod1_csim = as.mcmc(do.call(rbind, mod1_sim))
raftery.diag(mod1_sim)
summary(mod1_sim)
(pm_coef = colMeans(mod2_csim))
(pm_coef = colMeans(mod1_csim))
pm_coef[5]
Xb = pm_coef[1,5]
Xb = pm_coef[,5]
pm_coef[5]*1
Xb = pm_coef[5] + pm_coef[1]*60 + pm_coef[2]*0 + pm_coef[3]*50 + pm_coef[4]*0
Xb
phat = 1.0 / (1.0 + exp(-Xb))
phat
phat
str(data_jags)
pm_Xb = pm_coef["int"] + data_jags[,c(1:4)] %*% pm_coef[1:4]
data_jags
pm_Xb = pm_coef["int"] + data_jags[,c(1,2,3,4)] %*% pm_coef[1:4]
data_jags[,c(1,2,3,4)]
c(1,2,3,4)
data_jags[,1]
str(dat)
x = model.matrix(dat)[-1]
model.matrix(dat)
dat
head(X)
pm_Xb = pm_coef["int"] + X %*% pm_coef[1:4]
phat = 1.0 / (1.0 + exp(-pm_Xb))
phat
pm_Xb
pm_coef["b0"]
pm_Xb = pm_coef["b0"] + X %*% pm_coef[1:4]
pm_Xb
phat = 1.0 / (1.0 + exp(-pm_Xb))
phat
(tab0.7 = table(phat > 0.7, (dat$Correct / dat$Trials) > 0.7))
sum(diag(tab0.7)) / sum(tab0.7)
x1=0.8
x2=1.2
B0=1.5
B1=-0.3
B2=1.0
loglam = b0 + b1*x1 + b2*x2
loglam = B0 + B1*x1 + B2*x2
loglam
Ey = exp(loglam)
Ey
library("COUNT")
install.packages("COUNT")
library("COUNT")
data("badhealth")
?badhealth
head(badhealth)
mod_string = " model {
for (i in 1:length(numvisit)) {
numvisit[i] ~ dpois(lam[i])
log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]
}
int ~ dnorm(0.0, 1.0/1e6)
b_badh ~ dnorm(0.0, 1.0/1e4)
b_age ~ dnorm(0.0, 1.0/1e4)
b_intx ~ dnorm(0.0, 1.0/1e4)
} "
set.seed(102)
data_jags = as.list(badhealth)
params = c("int", "b_badh", "b_age", "b_intx")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
## convergence diagnostics
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
## compute DIC
dic = dic.samples(mod, n.iter=1e3)
mod2_string = " model {
for (i in 1:length(numvisit)) {
numvisit[i] ~ dpois(lam[i])
log(lam[i]) = int + b_badh*badh[i] + b_age*age[i]
}
int ~ dnorm(0.0, 1.0/1e6)
b_badh ~ dnorm(0.0, 1.0/1e4)
b_age ~ dnorm(0.0, 1.0/1e4)
} "
params2 = c("int", "b_badh", "b_age")
mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
update(mod2, 1e3)
mod_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))
## compute DIC
dic2 = dic.samples(mod2, n.iter=1e3)
dic2
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
dic
mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
update(mod2, 1e3)
mod2_sim = coda.samples(model=mod2,
variable.names=params2,
n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))
dic2 = dic.samples(mod2, n.iter=1e3)
dic
dic2
?rpois
b1 = rnorm(10000, 30)
mean(b1 > 22)
b1
setwd("~/src/BSTM/week4")
dat = read.csv(file="callers.csv", header=TRUE)
pairs(dat)
?boxplot
str(dat)
boxplot(calls ~ isgroup2, data=dat)
mod_string = " model {
for (i in 1:length(calls)) {
calls[i] ~ dpois( days_active[i] * lam[i] )
log(lam[i]) = b0 + b[1]*age[i] + b[2]*isgroup2[i]
}
b0 ~ dnorm(0.0, 1.0/1e6)
for (j in 1:2) {
b[j] ~ dnorm(0.0, 1.0/10.0^2)
}
} "
data_jags = as.list(dat)
params = c("b0", "b")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
#Q7
mod_string = " model {
for (i in 1:length(calls)) {
calls[i] ~ dpois( days_active[i] * lam[i] )
log(lam[i]) = b0 + b[1]*age[i] + b[2]*isgroup2[i]
}
b0 ~ dnorm(0.0, 1.0/10.0^2)
for (j in 1:2) {
b[j] ~ dnorm(0.0, 1.0/10.0^2)
}
} "
data_jags = as.list(dat)
params = c("b0", "b")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=10e4)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
## convergence diagnostics
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
## Analysis
summary(mod_sum)
summary(mod_sim)
b1 = rpois(10000, 30)
mean(b1 > 22)
?ppois
ppois(21, 30)
1 - ppois(21, 30)
1 - ppois(22, 30)
ppois(21, 30)
